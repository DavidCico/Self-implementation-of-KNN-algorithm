{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all functions necessary for the notebook\n",
    "import csv\n",
    "from random import randrange\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Loading, and conversion of CSV file #####\n",
    "\n",
    "## Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "## Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "## Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "\n",
    "##### Normalize Data ###########\n",
    "\n",
    "# Find the min and max values for each column\n",
    "\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    for i in range(len(dataset[0])):\n",
    "        colvalues = [row[i] for row in dataset]\n",
    "        min_value = min(colvalues) \n",
    "        max_value = max(colvalues)\n",
    "        minmax.append([min_value, max_value])\n",
    "    return minmax\n",
    "\n",
    "# Normalize the dataset except last row for classification values\n",
    "def Normalize_Dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting dataset methods ###\n",
    "\n",
    "# Split a dataset into a train and test set\n",
    "def train_test_split(dataset, split):\n",
    "    train = list()\n",
    "    train_size = split * len(dataset)\n",
    "    dataset_copy = list(dataset)\n",
    "    while len(train) < train_size:\n",
    "        index = randrange(len(dataset_copy))\n",
    "        train.append(dataset_copy.pop(index))\n",
    "    return train, dataset_copy\n",
    "\n",
    "# Split a dataset into $k$ folds\n",
    "def cross_validation_split(dataset, folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / folds)\n",
    "    for i in range(folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Accuracy for classification problems ######\n",
    "\n",
    "# Get accuracy of prediction #\n",
    "def getAccuracy(actual,predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i][-1] == predicted[i]:\n",
    "            correct += 1\n",
    "    return (correct / float(len(actual))) * 100.00\n",
    "\n",
    "# Calculate a Confusion Matrix #\n",
    "def confusion_matrix(actual, predicted):\n",
    "    unique = set([row[-1] for row in actual])\n",
    "    matrix = [list() for x in range(len(unique))]\n",
    "    for i in range(len(unique)):\n",
    "        matrix[i] = [0 for x in range(len(unique))]\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for i in range(len(actual)):\n",
    "        x = lookup[actual[i][-1]]\n",
    "        y = lookup[predicted[i]]\n",
    "        matrix[x][y] += 1\n",
    "    return unique, matrix\n",
    "\n",
    "# Printing a confusion matrix\n",
    "def print_confusion_matrix(unique, matrix):\n",
    "    print('Unique prediction values:')\n",
    "    print('(P)' + ' '.join(str(x) for x in unique))\n",
    "    print('(A)---')\n",
    "    print(\"Confusion Matrix:\")\n",
    "    for i, x in enumerate(unique):\n",
    "        print(\"%s| %s\" % (x, ' '.join(str(x) for x in matrix[i])))\n",
    "\n",
    "# Recall classification estimator #\n",
    "def recall_precision_calc(matrix):\n",
    "    for i in range(len(matrix[0])):\n",
    "        row_values = matrix[i] # row values of matrix\n",
    "        col_values = [row[i] for row in matrix] # column values of matrix\n",
    "        tp = col_values[i]\n",
    "        fp = sum(row_values)-row_values[i] # sum all row values - ones in diagonal\n",
    "        fn = sum(col_values)-col_values[i] # sum all col values - ones in diagonal\n",
    "    \n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return recall, precision, F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Distances definition ######\n",
    "\n",
    "#Euclidean Distance\n",
    "def EuclideanDistance(instance1, instance2, length):\n",
    "    distance = 0\n",
    "    for i in range(length):\n",
    "        distance += pow(instance2[i]-instance1[i],2)\n",
    "    return math.sqrt(distance)\n",
    "\n",
    "#Manhattan Distance\n",
    "def ManhattanDistance(instance1, instance2, length):\n",
    "    distance = 0\n",
    "    for i in range(length):\n",
    "        distance += abs(instance2[i]-instance1[i])\n",
    "    return distance\n",
    "\n",
    "#Minkowski distance with parameter p for power \n",
    "def MinkowskiDistance(instance1, instance2, length, p):\n",
    "    distance = 0\n",
    "    for i in range(length):\n",
    "        distance += pow(abs(instance2[i]-instance1[i]), p)\n",
    "    return pow(distance, 1/p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get neighbors\n",
    "def getNeighbors(trainingSet, testInstance, num_neighbors, distancetype, *args):\n",
    "    distances = []\n",
    "    length = len(testInstance)-1\n",
    "    for i in range(len(trainingSet)):\n",
    "        if distancetype == \"Euclidean\":\n",
    "            dist = EuclideanDistance(testInstance, trainingSet[i], length)\n",
    "        elif distancetype == \"Manhattan\":\n",
    "            dist = ManhattanDistance(testInstance, trainingSet[i], length)\n",
    "        else:\n",
    "            dist = MinkowskiDistance(testInstance, trainingSet[i], length, *args)\n",
    "        distances.append((trainingSet[i],dist))\n",
    "    distances.sort(key=operator.itemgetter(1))\n",
    "    #return distances\n",
    "    neighbors = []\n",
    "    for x in range(num_neighbors):\n",
    "        neighbors.append(distances[x][0])\n",
    "    return neighbors\n",
    "\n",
    "#Classification from neighbors (Classification problem)\n",
    "def getResponse(neighbors):\n",
    "    classVotes = {}\n",
    "    for x in range(len(neighbors)):\n",
    "        response = neighbors[x][-1]\n",
    "        if response in classVotes:\n",
    "            classVotes[response] += 1\n",
    "        else:\n",
    "            classVotes[response] = 1\n",
    "    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedVotes[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file iris.csv with 150 rows and 5 columns\n",
      "First line of dataset:  ['5.1', '3.5', '1.4', '0.2', 'Iris-setosa']\n",
      "First line of dataset with class defined by integer:  [5.1, 3.5, 1.4, 0.2, 2]\n",
      "\n",
      "Dictionary of lookup classes:  {'Iris-versicolor': 0, 'Iris-virginica': 1, 'Iris-setosa': 2}\n",
      "\n",
      "\n",
      "Algorithm solving:\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=2, actual=2\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=0, actual=0\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=0, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "> predicted=1, actual=1\n",
      "Accuracy :98.33333333333333%\n",
      "\n",
      "\n",
      "Unique prediction values:\n",
      "(P)0 1 2\n",
      "(A)---\n",
      "Confusion Matrix:\n",
      "0| 21 0 0\n",
      "1| 1 21 0\n",
      "2| 0 0 17\n",
      "\n",
      "\n",
      "Recall: 1.0\n",
      "Precision: 1.0\n",
      "F1 score: 1.0\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    # Load iris dataset\n",
    "    filename = 'iris.csv'\n",
    "    dataset = load_csv(filename)\n",
    "    print('Loaded data file {0} with {1} rows and {2} columns'.format(filename, len(dataset), len(dataset[0])))\n",
    "    print('First line of dataset: ', dataset[0])\n",
    "\n",
    "    # convert string columns to float\n",
    "    for i in range(4):\n",
    "        str_column_to_float(dataset, i)\n",
    "    # convert class column to int\n",
    "    lookup = str_column_to_int(dataset, 4)\n",
    "    print('First line of dataset with class defined by integer: ', dataset[0])\n",
    "    print('')\n",
    "    print('Dictionary of lookup classes: ', lookup)\n",
    "    print('\\n')\n",
    "    \n",
    "    # normalization of dataset\n",
    "    minmax = dataset_minmax(dataset)\n",
    "    Normalize_Dataset(dataset, minmax)\n",
    "\n",
    "    # Splitting dataset between Training and Testing Set\n",
    "    split = 0.6\n",
    "    trainingSet, testSet = train_test_split(dataset, split)\n",
    "\n",
    "    #generate predictions\n",
    "    print('Algorithm solving:')\n",
    "    predictions = []\n",
    "    num_neighbors = 3\n",
    "    for i in range(len(testSet)):\n",
    "        neighbors = getNeighbors(trainingSet, testSet[i], num_neighbors, \"Euclidean\")\n",
    "        classify = getResponse(neighbors)\n",
    "        predictions.append(classify)\n",
    "        print('> predicted=' + repr(classify) + ', actual=' + repr(testSet[i][-1]))\n",
    "\n",
    "    #Accuracy Assessment\n",
    "    accuracy = getAccuracy(testSet,predictions)\n",
    "    print('Accuracy :' + repr(accuracy) + '%')\n",
    "    unique, matrix = confusion_matrix(testSet, predictions)\n",
    "\n",
    "    print('\\n')\n",
    "    print_confusion_matrix(unique, matrix)\n",
    "    print('\\n')\n",
    "\n",
    "    #Calculate properties for recall and precision\n",
    "    Recall, Precision, F1_score = recall_precision_calc(matrix)\n",
    "    print('Recall:', Recall)\n",
    "    print('Precision:', Precision)\n",
    "    print('F1 score:', F1_score)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
